{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Harte Fractional Brownian Motion from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance**\n",
    "\n",
    "Covariance is a statistical measure of the tendancy of two random variables to move together.\n",
    "\n",
    "$Cov(X,Y) = \\frac{1}{n}\\sum_{i=1}^n (X - \\mu_X)(Y - \\mu_Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)]$\n",
    "\n",
    "In this context, $X, Y$ are random variables with population means $\\mu_X, \\mu_Y$ respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autocovariance Function**\n",
    "\n",
    "Autocovariance is an implementation of covariance on a time series; specifically, it looks at a random variable's tendancy to move relative to its *lags* (previous points or observations in the time series).\n",
    "\n",
    "$\\gamma_{k} = Cov(X_t, X_{t-k})$ where the covariance can be computed as above.\n",
    "\n",
    "Here $X_t$ is a time series (ice cream sales, stock price, etc.) where $t = 0, 1, 2, 3, \\dots$ it follows naturally, but is needed explicitly in some cases, if the lag exceeds the size of the series (say $N$) then $\\gamma_{n} = 0$ $\\forall n > N$.\n",
    "\n",
    "It should be noted that the autocovariance function exhibits symmetry as $\\gamma_k = \\gamma_{-k}$ which can be easily proven by applying the definition of covariance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Variance-Covariance & Autocovariance Matrices**\n",
    "\n",
    "A common representation of data, including time series, is given by a matrix. Features can be expressed in columns where rows represent observations. In a time series or similar panel data, rows typically represent a path (ice cream sales, stock prices, etc.) while columns represent points in time. The Variance-Covariance matrix tells us how features tend together while the Autocovariance matrix is a specific type of Variance-Covariance matrix that computes autocovariances at all relevant lags. \n",
    "\n",
    "They are similar structures but are constructed in different ways. There is a simple procedure for generating the autocovariance matrix, a more formal proof and technical discussion can be found: https://medium.com/quant-guild/when-is-the-matrix-product-the-variance-covariance-matrix-ffa047df7d19.\n",
    "\n",
    "\n",
    "Consider Variance-Covariance matrix $\\Sigma$ and data matrix $X$\n",
    "\n",
    "$\\Sigma = \\frac{(X - \\mu_X)(X - \\mu_X)^T}{n-1}$\n",
    "\n",
    "$\\Sigma$ will thus contain a diagonal of variances of each respective feature and covariances of features with each other.\n",
    "\n",
    "If we apply this methodolgy to a time series we will *not* get the autocovariance matrix as we will be generating point wise covariances - not covariances at general lags. Typically, we do this iteratively for the lags themselves while they are possible.\n",
    "\n",
    "$\\Gamma$ is typically used to denote this autocovariance matrix, consider the following case of lags out to 2...\n",
    "\n",
    "$$ \\Gamma = \n",
    "\\begin{bmatrix}\n",
    "\\gamma_0 & \\gamma_1 & \\gamma_2 \\\\\n",
    "\\gamma_1 & \\gamma_0 & \\gamma_1 \\\\\n",
    "\\gamma_2 & \\gamma_1 & \\gamma_0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In general,\n",
    "\n",
    " $$ \\Gamma = \n",
    " \\begin{bmatrix}\n",
    " \\gamma_0 & \\gamma_1 & \\gamma_2 & \\cdots & \\gamma_{N-1} \\\\\n",
    " \\gamma_1 & \\gamma_0 & \\gamma_1 & \\cdots & \\gamma_{N-2} \\\\\n",
    " \\gamma_2 & \\gamma_1 & \\gamma_0 & \\cdots & \\gamma_{N-3} \\\\\n",
    " \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " \\gamma_{N-1} & \\gamma_{N-2} & \\gamma_{N-3} & \\cdots & \\gamma_0\n",
    " \\end{bmatrix}\n",
    " $$\n",
    "\n",
    "\n",
    "Herein we will consider it the autocovariance matrix as we will be working with time series (Gaussian processes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toeplitz Matrices**\n",
    "\n",
    "A property of *stationary* time series is that their autocovariances are constant over time. In other words, their autocovariance function values will not change throughout the evolution of the process. Unlike the previously defined autocovariance matrix, the Toeplitz matrix *assumes* the top-left to bottom-right diagonals are constant.\n",
    "\n",
    " $$ T = \n",
    " \\begin{bmatrix}\n",
    " \\gamma_0 & \\gamma_1 & \\gamma_2 \\\\\n",
    " \\gamma_1 & \\gamma_0 & \\gamma_1 \\\\\n",
    " \\gamma_2 & \\gamma_1 & \\gamma_0\n",
    " \\end{bmatrix}\n",
    " $$\n",
    " \n",
    "where $\\gamma_0, \\gamma_1, \\gamma_2 \\in \\mathbb{R}$ are constant real numbers representing the autocovariance at lags 0, 1, and 2 respectively\n",
    "\n",
    "In terms of referencing the individual elements themselves it isn't uncommon to see notation as $T_{i,j} = \\gamma_{i-j}$ where the entry in the ith and jth column produce, in this structure, constant autocovariance $\\gamma_{i-j}$.\n",
    "\n",
    "Grenander, U., & Szegő, G. (1958). *Toeplitz Forms and Their Applications*. University of California Press.\n",
    "\n",
    "Grenander, U. (1954). *Stochastic Processes and Statistical Inference*. Arkiv för Matematik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Diagonalization**\n",
    "\n",
    "A square matrix (fortunately, all of the matrices that deal with variance-covariance structures are typically square) can be expressed in terms of its eigenvalues and eigenvectors; the so called Eigendecomposition or spectral decomposition. Other requirements are necessary including linearly independent eigenvectors - but we won't worry about these cases for the sake of this article (maybe they will be in the appendix if I can get to it :D).\n",
    "\n",
    "A square matrix (variance-coveriance matrix, autocovariance matrix) $A \\in \\mathbb{R}^{n \\times n}$ may be represented as $A = P D P^{-1}$ where $D$ is a diagonal matrix containing eigenvalues of $A$, $P$ is a matrix whose columns are eigenvectors of $A$.\n",
    "\n",
    "Clearly, once a procedure is defined its easy to determine if the solution is correct by taking the product. To diagonalize a matrix we look to solve the *characteristic equation*\n",
    "\n",
    "$$det(A - \\lambda I) = 0$$\n",
    "\n",
    "where $det(.)$ is the matrix determinate, we first solve for the eigenvalues then for each eigen value $\\lambda_i$ we solve \n",
    "\n",
    "$$(A - \\lambda_i I) = 0$$\n",
    "\n",
    "where non-zero solutions for $v_i$ yield the corresponding eigenvector to $\\lambda_i$\n",
    "\n",
    "Note: $P$ is a collection of eigenvectors (a matrix) and $D = diag(\\lambda)$ where $\\lambda = [\\lambda_1, \\lambda_2, \\dots]$ is a vector of eigenvalues placed on the diagonal of a matrix of comparable dimensionality. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrete Fourier Transform**\n",
    "\n",
    "Quite a fun topic, the Discrete Fourier Transform (DFT) transforms a vector from the time (spatial) domain to the frequency domain.\n",
    "\n",
    "Suppose $x = [x_0, x_1, \\dots, x_{n-1}] \\in \\mathbb{C}^n$\n",
    "\n",
    "The DFT is given by\n",
    "\n",
    "$$\\hat{x}_k = \\sum_{j=0}^{n-1} x_j e^{\\frac{-2 \\pi i j k}{n}}$$\n",
    "\n",
    "which operates fundamentally by changing the basis from standard basis vectors to complex exponentials.\n",
    "\n",
    "We can invert this transformation from the frequency domain back to the time domain by the inverse DFT (IDFT)\n",
    "\n",
    "$$x_j = \\frac{1}{n}\\sum_{k=0}^{n-1} \\hat{x}_k e^{\\frac{2 \\pi i j k}{n}}$$\n",
    "\n",
    "\n",
    "\n",
    "Cooley, J. W., & Tukey, J. W. (1965). *An Algorithm for the Machine Calculation of Complex Fourier Series*. Mathematics of Computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft(x : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Discrete Fourier Transform of a vector x.\n",
    "    \n",
    "    The Discrete Fourier Transform (DFT) transforms a vector from the time (spatial) domain to the frequency domain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Input vector to transform\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Transformed vector in frequency domain\n",
    "    \"\"\"\n",
    "\n",
    "    xk = np.zeros(len(x), dtype=np.complex128)\n",
    "    \n",
    "    for k in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            xk[k] += x[j] * np.exp(-2j * np.pi * k * j / len(x))\n",
    "    return xk\n",
    "\n",
    "def idft(xk : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Inverse Discrete Fourier Transform of a vector xk.\n",
    "    \n",
    "    The Inverse Discrete Fourier Transform (IDFT) transforms a vector from the frequency domain back to the time domain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xk : array-like\n",
    "        Input vector to transform\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Transformed vector in time domain\n",
    "    \"\"\"\n",
    "\n",
    "    xj = np.zeros(len(xk), dtype=np.complex128)\n",
    "    for j in range(len(xk)):\n",
    "        for k in range(len(xk)):\n",
    "            xj[j] += xk[k] * np.exp(2j * np.pi * k * j / len(xk))\n",
    "    return xj / len(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time domain to frequency domain and back, confirming the DFT and its inverse IDFT are correct\n",
    "assert (idft(dft([1, 2, 3, 4, 5])).real.astype(int) == [1, 2, 3, 4, 5]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrete Fourier Transform as a Matrix**\n",
    "\n",
    "The DFT may be viewed as a matrix and the DFT and IDFT as matrix-vector products.\n",
    "\n",
    "$F \\in \\mathbb{C}^{n \\times n}$ where $F_{j,k} = \\omega^{jk}$ and $\\omega = e^{\\frac{-2 \\pi i}{n}}$\n",
    "\n",
    "Where $F$ is the DFT Matrix, it is common to see many summations represented as matrix-vector products not just avoiding loops but simplifying the overall implementation.\n",
    "\n",
    "That is $$\\hat{x} = Fx$$ and $$x = \\frac{1}{n} F^{-1} \\hat{x}$$ where $x$ is the vector in the time (spatial) domain and $\\hat{x}$ is the vector in the frequency domain.\n",
    "\n",
    "It is also useful to note $F$ is unitary up to scale with $F^{-1} = \\frac{1}{n}F^*$\n",
    "\n",
    "This means that the DFT matrix $F$ has the special property that its inverse is proportional to its conjugate transpose.\n",
    "\n",
    "Specifically, $F^{-1} = \\frac{1}{n}F^*$ where $F^*$ is the conjugate transpose of $F$ and $n$ is the size of the matrix.\n",
    "\n",
    "This property makes computing the inverse DFT very efficient since we only need to take the conjugate transpose and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft_matrix(x : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Discrete Fourier Transform of a vector x.\n",
    "    \n",
    "    The Discrete Fourier Transform (DFT) transforms a vector from the time (spatial) domain to the frequency domain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Input vector to transform\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Transformed vector in frequency domain\n",
    "    \"\"\"\n",
    "\n",
    "    # the DFT matrix\n",
    "    F = np.zeros((len(x), len(x)), dtype=np.complex128)\n",
    "\n",
    "    for k in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            F[k, j] = np.exp(-2j * np.pi * k * j / len(x))\n",
    "    \n",
    "    return F\n",
    "\n",
    "def idft_matrix(x : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Inverse Discrete Fourier Transform of a vector x.\n",
    "    \n",
    "    The Inverse Discrete Fourier Transform (IDFT) transforms a vector from the frequency domain back to the time domain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Input vector to transform\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Transformed vector in time domain\n",
    "    \"\"\"\n",
    "\n",
    "    # DFT matrix\n",
    "    F = dft_matrix(x)\n",
    "\n",
    "    # using the property that the inverse matrix is the conjugate of F scaled\n",
    "    F_star = np.zeros((len(x), len(x)), dtype=np.complex128)\n",
    "    for k in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            F_star[k, j] = F[j, k].conjugate()\n",
    "\n",
    "    return F_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example implementation of DFT and IDFT as matrix-vector products\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "F = dft_matrix(x) # DFT matrix\n",
    "F_star = idft_matrix(x) # IDFT matrix\n",
    "\n",
    "freq_domain = F @ x\n",
    "\n",
    "# assess if the matrix-vector product matches the basic DFT implementation\n",
    "assert np.allclose(freq_domain, dft(x), rtol=1e-10, atol=1e-10)\n",
    "\n",
    "time_domain = (1 / len(x)) * F_star @ freq_domain\n",
    "\n",
    "# assess if the matrix-vector product matches the basic IDFT implementation\n",
    "assert np.allclose(time_domain.real, idft(dft(x)).real, rtol=1e-10, atol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Circulant Matrix**\n",
    "\n",
    "Quite an appropriate name, the circulant matrix is defined entirely by its first row $C \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "Each row in the subsequent circulant matrix is a right rotation of the row right above it (shift all the elements to the right by one each step down in the matrix). \n",
    "\n",
    "Where $c = [c_0, c_1, \\dots, c_{n-1}]$ and fully specifies $C$ it follows\n",
    "\n",
    " $$ C = circ(c) =\n",
    " \\begin{bmatrix}\n",
    " c_0 & c_1 & c_2 & \\cdots & c_{n-1} \\\\\n",
    " c_{n-1} & c_0 & c_1 & \\cdots & c_{n-2} \\\\\n",
    " c_{n-2} & c_{n-1} & c_0 & \\cdots & c_{n-3} \\\\\n",
    " \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " c_1 & c_2 & c_3 & \\cdots & c_0\n",
    " \\end{bmatrix}\n",
    " $$\n",
    "\n",
    " Now, we can do Eigendecomposition (Cholesky) to acheive the eigenvalues - however this is quite slow in $O(n^3)$.\n",
    " \n",
    "Davis, P. J. (1970). *Circulant Matrices*. Wiley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating a Gaussian Vector with Covariance Structure\n",
    "\n",
    "If the goal is to construct a random vector $X$ such that $\\mathbb{E} = 0$ with $Cov(X) = \\Gamma$\n",
    "\n",
    "We must find a matrix $A$ such that $\\Gamma = AA^T \\implies X = AZ$ where $Z \\sim N(0, I)$\n",
    "\n",
    "*Note:* this is quite like the univariate case of $\\sigma^2 = a \\implies X = \\sigma Z$ where $Z \\sim N(0, 1)$ and $a \\in \\mathbb{R}$ is the desired variance of process $X$\n",
    "\n",
    "There are several ways to accomplish this task:\n",
    "- Cholesky decomposition\n",
    "- Square root via eigendecomposition\n",
    "- Circulant diagonalization (FFT)\n",
    "\n",
    "*Why eigenvalues?*\n",
    "\n",
    "The eigenvalues tell us the amount of variance that exists in each principal direction of the Gaussian distribution. More specifically, the Gaussian here ($Z$) is elliptical in high dimensions with shape and spread dictated by eigenvalues (stretch) and eigenvectors (direction).\n",
    "\n",
    "*Actual Implementation*\n",
    "\n",
    "1.) Decompose $\\Gamma = Q \\Lambda Q^T$\n",
    "\n",
    "2.) Generate $Z \\sim N(0, I)$\n",
    "\n",
    "3.) Set $X = Q \\Lambda ^{1/2} Z$ (This is where the eigenvectors $Q$ and scaled eigenvalues $\\sqrt{\\lambda}$ come in to play)\n",
    "\n",
    "This should look familiar as if we want the scaling by $\\lambda$ in the variance sense we will need to scale by $\\sqrt{\\lambda}$\n",
    "\n",
    "*Remark:*\n",
    "\n",
    "This doesn't work magically...\n",
    "\n",
    "$$Cov(X) = Cov(Q \\Lambda^{1/2} Z) = Q \\Lambda^{1/2} \\mathbb{E}[ZZ^T] \\Lambda^{1/2}Q^T = Q \\Lambda^{1/2} I \\Lambda^{1/2}Q^T = Q \\Lambda Q^T = \\Gamma$$\n",
    "\n",
    "This yields the Gaussian vector $X$ with the desired covariance structure. The \"challenge\" then becomes determining *what* the covariance structure *should* be and how efficiently can we diagonalize it. The $Cov(Z) = \\mathbb{E}[ZZ^T]$ works to compute covariance here as $Z$ is a zero mean process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fractional Gaussian Noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
